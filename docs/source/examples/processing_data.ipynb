{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{tip}\n",
    "All of the {{ presets }} metioned below can be set during instantiation:\n",
    "```python\n",
    "magdata = MagentroData(..., presets={...})\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magentropy import MagentroData\n",
    "\n",
    "magdata = MagentroData('magdata.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping\n",
    "\n",
    "Before smoothing, the magnetization data must be grouped by field.\n",
    "Normally, the measured fields are not exact, so groups must be inferred.\n",
    "The method {{ test_grouping }} can be used to test grouping presets prior to\n",
    "fully processing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping_presets, grouped_by = magdata.test_grouping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With no arguments passed, the defaults in {{ presets }} are used. The method returns a dictionary\n",
    "of the grouping presets used to perform the grouping and a {mod}`pandas` `DataFrameGroupBy` object\n",
    "to see the results. In particular, the\n",
    "{meth}`DataFrameGroupBy.count() <pandas.core.groupby.DataFrameGroupBy.count>`\n",
    "method is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping_presets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by['T'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see that the default grouping presets are an empty array of `fields`, a `decimal` place\n",
    "of 5 for rounding, and infinite `max_diff`. Detailed information on each of these can be found in\n",
    "the {{ process_data }} documentation.\n",
    "\n",
    "In this instance, the presets direct the grouping method to group the fields simply by rounding to\n",
    "the 5th decimal place, which accurately determines the field groups, as shown by the `count()`\n",
    "method. There are five fields, each with 100 temperature measurements. In most cases, grouping\n",
    "by rounding should be sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing\n",
    "\n",
    "```{admonition} Reference\n",
    "J. J. Stickel, [Comput. Chem. Eng. 34, 467 (2010)](https://doi.org/10.1016/j.compchemeng.2009.10.007)\n",
    "```\n",
    "\n",
    "There are a number of options to control the smoothing. The default presets have been chosen\n",
    "sensibly but can be easily changed. All parameters, including grouping parameters, can be either\n",
    "set as new defaults using {{ presets }} or {{ set_presets }}, or used for a single {{ process_data }}\n",
    "run by entering an argument in {{ process_data }}. See the documentation for a complete\n",
    "description of all parameters. They are summarized below. The use of {{ set_presets }} is\n",
    "demonstrated in each case with the default presets purely for example; it is not necessary to set\n",
    "presets if the defaults are to be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output temperatures\n",
    "\n",
    "The smoothed magnetic moment will be evaulated at `npoints` evenly-spaced temperatures in the range\n",
    "`temp_range`. `npoints` expects an integer, and `temp_range` expects an {term}`array_like` of\n",
    "length 2. The default range `[-numpy.inf, numpy.inf]` adjusts to the maximum range in the data automatically.\n",
    "Additionally, only those fields with at least `min_sweep_len` measured temperatures in their\n",
    "respective temperature sweeps will be processed. The default is 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import inf\n",
    "\n",
    "magdata.set_presets(npoints=1000, temp_range=[-inf, inf], min_sweep_len=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "The two most important options for the regularization (smoothing) itself are the derivative order\n",
    "`d_order` and the regularization parameter {math}`\\lambda` for each field, `lmbds`.\n",
    "\n",
    "The derivative of the magnetic moment with respect to temperature of order `d_order` is used to\n",
    "quantify the \"roughness\" of the fitted curves. Generally, 2 or 3 work well. The default is 2.\n",
    "\n",
    "The regularization parameter determines the empahsis that is given to the roughness\n",
    "regularization penalty. A higher {math}`\\lambda` results in a smoother curve, and a {math}`\\lambda`\n",
    "of zero results in interpolation. A {math}`\\lambda` can be specified for each field\n",
    "(in increasing field order) as an {term}`array_like` of the same length as the number of fields.\n",
    "Any field with a corresponding {math}`\\lambda` of {data}`numpy.nan` will have an \"optimal\"\n",
    "{math}`\\lambda` determined automatically; see [below](#optimal-regularization-parameters).\n",
    "The default `lmbds` is an array with a single {data}`numpy.nan`, which indicates that an optimal\n",
    "{math}`\\lambda` should be found for each field. The same behavior occurs if an empty list\n",
    "is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magdata.set_presets(d_order=2, lmbds=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal regularization parameters\n",
    "\n",
    "Numerical optimization is used to determine the optimal regularization parameter for each field\n",
    "without a {math}`\\lambda` provided. Three metrics are available to quantify the meaning of\n",
    "\"optimal\":\n",
    "\n",
    "1. Generalized cross validation (GCV). The GCV variance is minimized. Set `match_err` to `False`\n",
    "(default).\n",
    "\n",
    "2. Error matching. The standard deviation of the absolute differences between the measured and\n",
    "smoothed magnetic moment points is matched to a value. The squared difference between the standard\n",
    "deviation and this value is minimized. Set `match_err` to a single value to match this value for\n",
    "all fields, an {term}`array_like` of the same length as the number of fields to match a different\n",
    "value for each field (in order of increasing field), or one of `'min'`, `'mean'`, or `'max'` to\n",
    "use the minimum, mean, or maximum value of the error column for each field as the value.\n",
    "\n",
    "3. Per-point error matching (experimental). The absolute differences between the measured and\n",
    "smoothed magnetic moment points are computed, and the sum of squared differences between these\n",
    "and the corresponding values in the error column is minimized. Set `match_err` to `True`.\n",
    "\n",
    "Each of these requires an initial guess, given by `lmbd_guess`. Currently, a single guess to use\n",
    "for all fields is supported. For control over the minimization, keyword arguments to pass\n",
    "to {func}`scipy.optimize.minimize` can be given as a dictionary to `min_kwargs`. Keep in mind\n",
    "that any values passed to `min_kwargs` should be with respect to {math}`\\log_{10} \\lambda`,\n",
    "since this is the value that is minimized internally. (However, `lmbd_guess` is the guess for\n",
    "{math}`\\lambda` itself; no {math}`\\log_{10}`.) Lastly, `weight_err` specifies whether to weight measurements by the\n",
    "normalized inverse squares of the errors. The default is `True`.\n",
    "\n",
    "See {{ process_data }} for full documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magdata.set_presets(\n",
    "    lmbd_guess=1e-4, weight_err=True, match_err=False,\n",
    "    min_kwargs = {\n",
    "        'method': 'Nelder-Mead',\n",
    "        'bounds': ((-inf, inf),),\n",
    "        'options': {'maxfev': 50, 'xatol': 1e-2, 'fatol': 1e-6}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating from zero field\n",
    "\n",
    "The calculation of entropy requires that the derivative of the magnetic moment with respect to\n",
    "temperature be integrated with respect to magnetic field, starting at zero field. Zero field\n",
    "measurements (with zero moment) are prepended before integration during processing, so it is not\n",
    "necessary to include zero field measurements in the input data.\n",
    "\n",
    "The zeros can be included in {{ processed_df }} if `add_zeros` is set to `True`. It is `False` by\n",
    "default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magdata.set_presets(add_zeros=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration\n",
    "\n",
    "Simple usage of {{ process_data }} is shown, including the adjustment of the regularization\n",
    "parameters by eye after they have been estimated initially. Plots are used to verify the\n",
    "success of the smoothing. See {doc}`plotting` for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "magdata.process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "mystnb": {
     "image": {
      "alt": "processing demonstration initial plot",
      "width": "100%"
     }
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "magdata.plot_lines(data_prop='M_per_mass', data_version='compare', ax=ax[0])\n",
    "\n",
    "magdata.plot_lines(\n",
    "    data_prop='dM_dT', data_version='compare', ax=ax[1], colorbar=True,\n",
    "    colorbar_kwargs={'ax': ax, 'fraction': 0.1, 'pad': 0.02}\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "The errors in this fake data are greatly exaggerated! Most instruments will have relative errors\n",
    "much smaller than those shown here.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the smoothed data (lines) looks much better than the raw data (dots), especially\n",
    "in the derivative plot on the right. Generalized cross validation has done a pretty good job of\n",
    "selecting optimal regularization parameters, which we can view using {{ last_presets }}:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magdata.last_presets['lmbds']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The {{ presets }} are the same as they were before; however, setting them to {{ last_presets }} is simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magdata.presets = magdata.last_presets\n",
    "magdata.presets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also adjust `lmbds` for a single run and re-process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "magdata.process_data(lmbds=[1e-4, 5e-5, 1e-4, 1e-5, 1e-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "mystnb": {
     "image": {
      "alt": "processing demonstration second plot",
      "width": "100%"
     }
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "magdata.plot_lines(data_prop='M_per_mass', data_version='compare', ax=ax[0])\n",
    "magdata.plot_lines(\n",
    "    data_prop='dM_dT', data_version='compare', ax=ax[1], colorbar=True,\n",
    "    colorbar_kwargs={'ax': ax, 'fraction': 0.1, 'pad': 0.02}\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error column in {{ processed_df }} will still be empty after all this.\n",
    "See {doc}`bootstrap_estimates`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('magentropy-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ab78a24dcd5d2f912eb3dd8dff5c36262a9ffc4f386b5a71d4924d641a6e82d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
